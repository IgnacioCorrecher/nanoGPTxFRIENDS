{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoGPT x Friends Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('data/Friends_Transcript.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the transcript is 4899189 chars. \n",
      "\n",
      "And the first 1000 chars are as follows: \n",
      "\n",
      "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
      "Written by: Marta Kauffman & David Crane\n",
      "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
      "Monica: There's nothing to tell! He's just some guy I work with!\n",
      "Joey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Chandler: All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
      "Phoebe: Wait, does he eat chalk?\n",
      "(They all stare, bemused.)\n",
      "Phoebe: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
      "Monica: Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\n",
      "Chandler: Sounds like a date to me.\n",
      "[Time Lapse]\n",
      "Chandler: Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\n",
      "All: Oh, yeah. Had that dream.\n",
      "Chandler: Then I look down, and I realize there's a phone... there.\n",
      "Joey: Instead of...?\n",
      "Chandler: That's right.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The length of the transcript is', len(text), 'chars. \\n\\nAnd the first 1000 chars are as follows: \\n')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we are dealing with almost 5M chars in the dataset, now we will create a vocabulary from this dataset using its unique characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab and vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(''.join(vocab))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a character encoder. When trying to represent text with numbers (integers) you have a couple of choices to make, there is a trade-off between shorter representations and bigger vocabularies. \n",
    "\n",
    "We will keep it simple and use a character encoding, i.e, we will give every char from out vocab a number and then we will map this dictionary to encode our text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 79, 85, 83, 69]\n",
      "house\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(vocab)}\n",
    "itos = {i:ch for i,ch in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"house\"))\n",
    "print(decode(encode(\"house\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we have a number for each char in the word we want to encode. \n",
    "\n",
    "Imagine we want need representations, let's suppose we are encoding the word _house_, with our encoder we would need an array with length 5, but if the encoder had an specific number for _ho_ and another one for _use_ we would simply need 2 numbers to represent the word. But in order to have those words in the vocab we would need bigger a vocab size, to match every possible combination of chars. This is the tradeoff we were talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors (yeah!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53, 41, 38,  1, 48, 47, 38,  1, 56, 41, 38, 51, 38,  1, 46, 48, 47, 42,\n",
       "        36, 34,  1, 40, 38, 53, 52,  1, 34,  1, 47, 38, 56,  1, 51, 48, 48, 46,\n",
       "        34, 53, 38,  1,  9, 53, 41, 38,  1, 49, 42, 45, 48, 53, 14, 53, 41, 38,\n",
       "         1, 54, 47, 36, 54, 53,  1, 55, 38, 51, 52, 42, 48, 47, 10,  0, 56, 82,\n",
       "        73, 84, 84, 69, 78,  1, 66, 89, 27,  1, 46, 65, 82, 84, 65,  1, 44, 65,\n",
       "        85, 70, 70, 77, 65, 78,  1,  7,  1, 37, 65, 86, 73, 68,  1, 36, 82, 65,\n",
       "        78, 69,  0, 60, 52, 67, 69, 78, 69, 27,  1, 36, 69, 78, 84, 82, 65, 76,\n",
       "         1, 49, 69, 82, 75, 13,  1, 36, 72, 65, 78, 68, 76, 69, 82, 13,  1, 43,\n",
       "        79, 69, 89, 13,  1, 49, 72, 79, 69, 66, 69, 13,  1, 65, 78, 68,  1, 46,\n",
       "        79, 78, 73, 67, 65,  1, 65, 82, 69,  1, 84, 72, 69, 82, 69, 15, 61,  0,\n",
       "        46, 79, 78, 73, 67, 65, 27,  1, 53, 72, 69, 82, 69,  8, 83,  1, 78, 79,\n",
       "        84, 72, 73, 78, 71,  1, 84, 79,  1, 84, 69, 76, 76,  2,  1, 41, 69,  8,\n",
       "        83,  1, 74, 85, 83, 84,  1, 83, 79, 77, 69,  1, 71, 85, 89,  1, 42,  1,\n",
       "        87, 79, 82, 75,  1, 87, 73, 84, 72,  2,  0, 43, 79, 69, 89, 27,  1, 36,\n",
       "         8, 77, 79, 78, 13,  1, 89, 79, 85,  8, 82, 69,  1, 71, 79, 73, 78, 71,\n",
       "         1, 79, 85, 84,  1, 87, 73, 84, 72,  1, 84, 72, 69,  1, 71, 85, 89,  2,\n",
       "         1, 53, 72, 69, 82, 69,  8, 83,  1, 71, 79, 84, 84, 65,  1, 66, 69,  1,\n",
       "        83, 79, 77, 69, 84, 72, 73, 78, 71,  1, 87, 82, 79, 78, 71,  1, 87, 73,\n",
       "        84, 72,  1, 72, 73, 77,  2,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1,\n",
       "        34, 76, 76,  1, 82, 73, 71, 72, 84,  1, 43, 79, 69, 89, 13,  1, 66, 69,\n",
       "         1, 78, 73, 67, 69, 15,  1, 52, 79,  1, 68, 79, 69, 83,  1, 72, 69,  1,\n",
       "        72, 65, 86, 69,  1, 65,  1, 72, 85, 77, 80, 32,  1, 34,  1, 72, 85, 77,\n",
       "        80,  1, 65, 78, 68,  1, 65,  1, 72, 65, 73, 82, 80, 73, 69, 67, 69, 32,\n",
       "         0, 49, 72, 79, 69, 66, 69, 27,  1, 56, 65, 73, 84, 13,  1, 68, 79, 69,\n",
       "        83,  1, 72, 69,  1, 69, 65, 84,  1, 67, 72, 65, 76, 75, 32,  0,  9, 53,\n",
       "        72, 69, 89,  1, 65, 76, 76,  1, 83, 84, 65, 82, 69, 13,  1, 66, 69, 77,\n",
       "        85, 83, 69, 68, 15, 10,  0, 49, 72, 79, 69, 66, 69, 27,  1, 43, 85, 83,\n",
       "        84, 13,  1,  8, 67, 65, 85, 83, 69, 13,  1, 42,  1, 68, 79, 78,  8, 84,\n",
       "         1, 87, 65, 78, 84,  1, 72, 69, 82,  1, 84, 79,  1, 71, 79,  1, 84, 72,\n",
       "        82, 79, 85, 71, 72,  1, 87, 72, 65, 84,  1, 42,  1, 87, 69, 78, 84,  1,\n",
       "        84, 72, 82, 79, 85, 71, 72,  1, 87, 73, 84, 72,  1, 36, 65, 82, 76, 14,\n",
       "         1, 79, 72,  2,  0, 46, 79, 78, 73, 67, 65, 27,  1, 48, 75, 65, 89, 13,\n",
       "         1, 69, 86, 69, 82, 89, 66, 79, 68, 89,  1, 82, 69, 76, 65, 88, 15,  1,\n",
       "        53, 72, 73, 83,  1, 73, 83,  1, 78, 79, 84,  1, 69, 86, 69, 78,  1, 65,\n",
       "         1, 68, 65, 84, 69, 15,  1, 42, 84,  8, 83,  1, 74, 85, 83, 84,  1, 84,\n",
       "        87, 79,  1, 80, 69, 79, 80, 76, 69,  1, 71, 79, 73, 78, 71,  1, 79, 85,\n",
       "        84,  1, 84, 79,  1, 68, 73, 78, 78, 69, 82,  1, 65, 78, 68, 14,  1, 78,\n",
       "        79, 84,  1, 72, 65, 86, 73, 78, 71,  1, 83, 69, 88, 15,  0, 36, 72, 65,\n",
       "        78, 68, 76, 69, 82, 27,  1, 52, 79, 85, 78, 68, 83,  1, 76, 73, 75, 69,\n",
       "         1, 65,  1, 68, 65, 84, 69,  1, 84, 79,  1, 77, 69, 15,  0, 60, 53, 73,\n",
       "        77, 69,  1, 45, 65, 80, 83, 69, 61,  0, 36, 72, 65, 78, 68, 76, 69, 82,\n",
       "        27,  1, 34, 76, 82, 73, 71, 72, 84, 13,  1, 83, 79,  1, 42,  8, 77,  1,\n",
       "        66, 65, 67, 75,  1, 73, 78,  1, 72, 73, 71, 72,  1, 83, 67, 72, 79, 79,\n",
       "        76, 13,  1, 42,  8, 77,  1, 83, 84, 65, 78, 68, 73, 78, 71,  1, 73, 78,\n",
       "         1, 84, 72, 69,  1, 77, 73, 68, 68, 76, 69,  1, 79, 70,  1, 84, 72, 69,\n",
       "         1, 67, 65, 70, 69, 84, 69, 82, 73, 65, 13,  1, 65, 78, 68,  1, 42,  1,\n",
       "        82, 69, 65, 76, 73, 90, 69,  1, 42,  1, 65, 77,  1, 84, 79, 84, 65, 76,\n",
       "        76, 89,  1, 78, 65, 75, 69, 68, 15,  0, 34, 76, 76, 27,  1, 48, 72, 13,\n",
       "         1, 89, 69, 65, 72, 15,  1, 41, 65, 68,  1, 84, 72, 65, 84,  1, 68, 82,\n",
       "        69, 65, 77, 15,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1, 53, 72, 69,\n",
       "        78,  1, 42,  1, 76, 79, 79, 75,  1, 68, 79, 87, 78, 13,  1, 65, 78, 68,\n",
       "         1, 42,  1, 82, 69, 65, 76, 73, 90, 69,  1, 84, 72, 69, 82, 69,  8, 83,\n",
       "         1, 65,  1, 80, 72, 79, 78, 69, 15, 15, 15,  1, 84, 72, 69, 82, 69, 15,\n",
       "         0, 43, 79, 69, 89, 27,  1, 42, 78, 83, 84, 69, 65, 68,  1, 79, 70, 15,\n",
       "        15, 15, 32,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1, 53, 72, 65, 84,\n",
       "         8, 83,  1, 82, 73, 71, 72, 84, 15,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converted the whole text into numbers using our encoder and store the value in a tensor object using *PyTorch*. Now we can check again the first 1000 chars from the dataset, but now converted to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(len(data)*0.95)\n",
    "train_ds = data[:train_n]\n",
    "test_ds = data[train_n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have almost 5M chars in the dataset, we will do a 95/5 train/test split, because 5% is more than enough to validate and we will need as much as we can in the train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram model attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      " tensor([[72, 79, 87,  1, 67, 79, 77, 69],\n",
      "        [75, 78, 79, 87, 15,  1, 42,  1],\n",
      "        [85,  1, 68, 79, 73, 78, 71,  1],\n",
      "        [79,  1, 42,  1, 72, 65, 86, 69]])\n",
      "targets:\n",
      " tensor([[79, 87,  1, 67, 79, 77, 69,  1],\n",
      "        [78, 79, 87, 15,  1, 42,  1, 65],\n",
      "        [ 1, 68, 79, 73, 78, 71,  1, 84],\n",
      "        [ 1, 42,  1, 72, 65, 86, 69,  1]])\n",
      "When input is [72], the target is: 79\n",
      "When input is [72, 79], the target is: 87\n",
      "When input is [72, 79, 87], the target is: 1\n",
      "When input is [72, 79, 87, 1], the target is: 67\n",
      "When input is [72, 79, 87, 1, 67], the target is: 79\n",
      "When input is [72, 79, 87, 1, 67, 79], the target is: 77\n",
      "When input is [72, 79, 87, 1, 67, 79, 77], the target is: 69\n",
      "When input is [72, 79, 87, 1, 67, 79, 77, 69], the target is: 1\n",
      "When input is [75], the target is: 78\n",
      "When input is [75, 78], the target is: 79\n",
      "When input is [75, 78, 79], the target is: 87\n",
      "When input is [75, 78, 79, 87], the target is: 15\n",
      "When input is [75, 78, 79, 87, 15], the target is: 1\n",
      "When input is [75, 78, 79, 87, 15, 1], the target is: 42\n",
      "When input is [75, 78, 79, 87, 15, 1, 42], the target is: 1\n",
      "When input is [75, 78, 79, 87, 15, 1, 42, 1], the target is: 65\n",
      "When input is [85], the target is: 1\n",
      "When input is [85, 1], the target is: 68\n",
      "When input is [85, 1, 68], the target is: 79\n",
      "When input is [85, 1, 68, 79], the target is: 73\n",
      "When input is [85, 1, 68, 79, 73], the target is: 78\n",
      "When input is [85, 1, 68, 79, 73, 78], the target is: 71\n",
      "When input is [85, 1, 68, 79, 73, 78, 71], the target is: 1\n",
      "When input is [85, 1, 68, 79, 73, 78, 71, 1], the target is: 84\n",
      "When input is [79], the target is: 1\n",
      "When input is [79, 1], the target is: 42\n",
      "When input is [79, 1, 42], the target is: 1\n",
      "When input is [79, 1, 42, 1], the target is: 72\n",
      "When input is [79, 1, 42, 1, 72], the target is: 65\n",
      "When input is [79, 1, 42, 1, 72, 65], the target is: 86\n",
      "When input is [79, 1, 42, 1, 72, 65, 86], the target is: 69\n",
      "When input is [79, 1, 42, 1, 72, 65, 86, 69], the target is: 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(777)\n",
    "context_length = 8 #How much are we feeding the transformer at the same time, also known as block size. In this case 8 numbers\n",
    "batch_size = 4 #Independent sequences processed in parallel\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_ds if split == 'train' else test_ds\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:\\n',xb)\n",
    "print('targets:\\n',yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_length):\n",
    "        context = xb[b, :t+1] # Row b, all columns until t+1 element (not included)\n",
    "        target = yb[b,t] #Row b, element t\n",
    "        print(f\"When input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 94])\n",
      "tensor(4.9358, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # We will focus on the last element because this is a Bigram Model, so we fix T = -1\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text using Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "g`3u`>(9]hrCJ7ET:}xUl%!RF]v$B+lJ)XCB9u\n",
      "#^IBNDeY2/XMYDUMv:qsoy#t0a^O29EO>hUfH^_N0a*o0%IpO*!( d\n",
      "15z#\"3\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.495844841003418\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for steps in range(100000):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bun NI's aie d Tothey, lothong g llleve acat adnde t an!\n",
      "Romera stou: aya  t, dld thit ot.y hears fi\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
